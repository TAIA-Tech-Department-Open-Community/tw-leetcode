name: Compile Dataset

on:
  push:
    branches: [ main ]
    paths:
      - '*/answer.ts'
      - '*/Note.md'
  schedule:
    # Run daily at 12:00 UTC
    - cron: '0 12 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  compile:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for all branches
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install dependencies
        run: |
          cd scripts
          npm install

      - name: Compile dataset
        run: node scripts/compile-dataset.js

      - name: Show dataset stats
        run: |
          echo "Dataset compilation completed!"
          echo "Dataset size: $(wc -l < data/datasets.jsonl) problems"
          echo "File size: $(du -h data/datasets.jsonl | cut -f1)"
          echo ""
          echo "Sample entries:"
          head -2 data/datasets.jsonl | jq '.'

      - name: Validate JSONL format
        run: |
          echo "Validating JSONL format..."
          node -e "
            const fs = require('fs');
            const readline = require('readline');
            
            async function validateJSONL() {
              const fileStream = fs.createReadStream('data/datasets.jsonl');
              const rl = readline.createInterface({
                input: fileStream,
                crlfDelay: Infinity
              });
              
              let lineNumber = 0;
              let validEntries = 0;
              let errors = 0;
              
              for await (const line of rl) {
                lineNumber++;
                try {
                  const data = JSON.parse(line);
                  if (!data.text || !data.question || !data.constraints || !data.thought || !data.answer || !data.src || !data.time_complexity || !data.space_complexity) {
                    console.error(\`Line \${lineNumber}: Missing required fields\`);
                    errors++;
                  } else {
                    validEntries++;
                  }
                } catch (e) {
                  console.error(\`Line \${lineNumber}: Invalid JSON - \${e.message}\`);
                  errors++;
                }
              }
              
              console.log(\`Validation complete: \${validEntries} valid entries, \${errors} errors\`);
              if (errors > 0) {
                process.exit(1);
              }
            }
            
            validateJSONL();
          "

      - name: Create dataset branch
        run: |
          # Create or switch to dataset branch
          git checkout -B dataset
          
          # Backup the compiled dataset and README before removing files
          cp data/datasets.jsonl datasets-backup.jsonl
          cp scripts/README.md.dataset readme-backup.md
          
          # Remove all files except the ones we want to keep
          git rm -rf . || true
          
          # Restore essential files from main
          git checkout main -- .gitattributes .gitignore || true
          
          # Create data directory and restore the dataset and README
          mkdir -p data
          mv datasets-backup.jsonl data/datasets.jsonl
          mv readme-backup.md README.md
          
          # Update .gitignore for dataset branch
          echo "# Dataset branch - only contains compiled data" > .gitignore
          echo "node_modules/" >> .gitignore
          echo "*.log" >> .gitignore
          echo ".DS_Store" >> .gitignore
          
          # Create .gitattributes for large files
          echo "*.jsonl filter=lfs diff=lfs merge=lfs -text" > .gitattributes
          echo "data/*.jsonl filter=lfs diff=lfs merge=lfs -text" >> .gitattributes
          
          # Add and commit files
          git add .
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git commit -m "Update dataset - $(date -u +%Y-%m-%d)" || echo "No changes to commit"
          
          # Push to dataset branch with proper authentication
          git push https://${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git dataset --force

      - name: Setup Python for Hugging Face
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Hugging Face Hub
        run: pip install huggingface_hub

      - name: Switch to dataset branch for upload
        run: git checkout dataset

      - name: Upload to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          # Use the newer hf CLI commands instead of deprecated huggingface-cli
          echo "Setting up Hugging Face CLI..."
          echo "$HF_TOKEN" | hf auth login --token-stdin
          
          # Check token permissions
          echo "Checking token permissions..."
          hf whoami
          
          echo "Current branch: $(git branch --show-current)"
          echo "Files in current directory:"
          find . -type f -not -path './.git/*' -not -path './scripts/*' | head -10
          
          # Create the repository if it doesn't exist using newer CLI
          echo "Creating/checking repository..."
          hf repo create twinkle-ai/tw-leetcode --type dataset --exist-ok || echo "Repository exists or creation skipped"
          
          # Try using the hf upload command which is more reliable
          echo "Uploading files using hf upload..."
          
          # Upload data directory
          if [ -d "data" ]; then
            hf upload twinkle-ai/tw-leetcode data/ data/ --repo-type dataset --commit-message "Auto-update dataset - $(date -u +%Y-%m-%d)"
            echo "Successfully uploaded data directory!"
          fi
          
          # Upload README
          if [ -f "README.md" ]; then
            hf upload twinkle-ai/tw-leetcode README.md README.md --repo-type dataset --commit-message "Auto-update README - $(date -u +%Y-%m-%d)"
            echo "Successfully uploaded README.md!"
          fi
          
          # Upload .gitattributes if exists
          if [ -f ".gitattributes" ]; then
            hf upload twinkle-ai/tw-leetcode .gitattributes .gitattributes --repo-type dataset --commit-message "Auto-update .gitattributes - $(date -u +%Y-%m-%d)"
            echo "Successfully uploaded .gitattributes!"
          fi
          
          echo "Dataset uploaded successfully to Hugging Face!"
