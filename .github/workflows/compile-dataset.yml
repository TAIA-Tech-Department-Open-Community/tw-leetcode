name: Compile Dataset

on:
  push:
    branches: [ main ]
    paths:
      - '*/answer.ts'
      - '*/Note.md'
  schedule:
    # Run daily at 12:00 UTC
    - cron: '0 12 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  compile:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for all branches
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install dependencies
        run: |
          cd scripts
          npm install

      - name: Compile dataset
        run: node scripts/compile-dataset.js

      - name: Show dataset stats
        run: |
          echo "Dataset compilation completed!"
          echo "Dataset size: $(wc -l < data/datasets.jsonl) problems"
          echo "File size: $(du -h data/datasets.jsonl | cut -f1)"
          echo ""
          echo "Sample entries:"
          head -2 data/datasets.jsonl | jq '.'

      - name: Validate JSONL format
        run: |
          echo "Validating JSONL format..."
          node -e "
            const fs = require('fs');
            const readline = require('readline');
            
            async function validateJSONL() {
              const fileStream = fs.createReadStream('data/datasets.jsonl');
              const rl = readline.createInterface({
                input: fileStream,
                crlfDelay: Infinity
              });
              
              let lineNumber = 0;
              let validEntries = 0;
              let errors = 0;
              
              for await (const line of rl) {
                lineNumber++;
                try {
                  const data = JSON.parse(line);
                  if (!data.text || !data.question || !data.constraints || !data.thought || !data.answer || !data.src || !data.time_complexity || !data.space_complexity) {
                    console.error(\`Line \${lineNumber}: Missing required fields\`);
                    errors++;
                  } else {
                    validEntries++;
                  }
                } catch (e) {
                  console.error(\`Line \${lineNumber}: Invalid JSON - \${e.message}\`);
                  errors++;
                }
              }
              
              console.log(\`Validation complete: \${validEntries} valid entries, \${errors} errors\`);
              if (errors > 0) {
                process.exit(1);
              }
            }
            
            validateJSONL();
          "

      - name: Create dataset branch
        run: |
          # Create or switch to dataset branch
          git checkout -B dataset
          
          # Backup the compiled dataset and README before removing files
          cp data/datasets.jsonl datasets-backup.jsonl
          cp scripts/README.md.dataset readme-backup.md
          
          # Remove all files except the ones we want to keep
          git rm -rf . || true
          
          # Restore essential files from main
          git checkout main -- .gitattributes .gitignore || true
          
          # Create data directory and restore the dataset and README
          mkdir -p data
          mv datasets-backup.jsonl data/datasets.jsonl
          mv readme-backup.md README.md
          
          # Update .gitignore for dataset branch
          echo "# Dataset branch - only contains compiled data" > .gitignore
          echo "node_modules/" >> .gitignore
          echo "*.log" >> .gitignore
          echo ".DS_Store" >> .gitignore
          
          # Create .gitattributes for large files
          echo "*.jsonl filter=lfs diff=lfs merge=lfs -text" > .gitattributes
          echo "data/*.jsonl filter=lfs diff=lfs merge=lfs -text" >> .gitattributes
          
          # Add and commit files
          git add .
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git commit -m "Update dataset - $(date -u +%Y-%m-%d)" || echo "No changes to commit"
          
          # Push to dataset branch with proper authentication
          git push https://${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git dataset --force

      - name: Setup Python for Hugging Face
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Hugging Face Hub
        run: pip install huggingface_hub

      - name: Switch to dataset branch for upload
        run: git checkout dataset

      - name: Upload to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python -c "
          import os
          from huggingface_hub import HfApi, create_repo, login
          from pathlib import Path
          import subprocess
          
          # Login with token
          try:
              login(token=os.environ['HF_TOKEN'])
              print('Successfully logged in to Hugging Face')
          except Exception as e:
              print(f'Login failed: {e}')
              exit(1)
          
          # Initialize HF API with token
          api = HfApi(token=os.environ['HF_TOKEN'])
          
          # Repository details
          repo_id = 'twinkle-ai/tw-leetcode'
          
          # Create repository if it doesn't exist (will skip if exists)
          try:
              create_repo(repo_id, repo_type='dataset', exist_ok=True, token=os.environ['HF_TOKEN'])
              print(f'Repository {repo_id} is ready')
          except Exception as e:
              print(f'Repository creation/check result: {e}')
          
          # Verify we're on dataset branch and list files
          branch_result = subprocess.run(['git', 'branch', '--show-current'], capture_output=True, text=True)
          print(f'Current branch: {branch_result.stdout.strip()}')
          
          # List files to upload (should only be clean dataset files)
          import glob
          files = glob.glob('**/*', recursive=True)
          files = [f for f in files if os.path.isfile(f) and not f.startswith('.git') and 'node_modules' not in f]
          print(f'Files to upload: {files}')
          
          # Upload files individually to avoid permission issues
          try:
              # Upload data directory
              if os.path.exists('data'):
                  api.upload_folder(
                      folder_path='data',
                      path_in_repo='data',
                      repo_id=repo_id,
                      repo_type='dataset',
                      commit_message=f'Auto-update dataset - {subprocess.check_output([\"date\", \"-u\", \"+%Y-%m-%d\"]).decode().strip()}',
                      token=os.environ['HF_TOKEN']
                  )
                  print('Successfully uploaded data folder!')
              
              # Upload README if exists
              if os.path.exists('README.md'):
                  api.upload_file(
                      path_or_fileobj='README.md',
                      path_in_repo='README.md',
                      repo_id=repo_id,
                      repo_type='dataset',
                      commit_message=f'Auto-update README - {subprocess.check_output([\"date\", \"-u\", \"+%Y-%m-%d\"]).decode().strip()}',
                      token=os.environ['HF_TOKEN']
                  )
                  print('Successfully uploaded README.md!')
              
              # Upload other root files (like .gitattributes, .gitignore) but exclude node_modules
              for file in files:
                  if file not in ['README.md'] and not file.startswith('data/') and 'node_modules' not in file and 'scripts/' not in file:
                      try:
                          api.upload_file(
                              path_or_fileobj=file,
                              path_in_repo=file,
                              repo_id=repo_id,
                              repo_type='dataset',
                              commit_message=f'Auto-update {file} - {subprocess.check_output([\"date\", \"-u\", \"+%Y-%m-%d\"]).decode().strip()}',
                              token=os.environ['HF_TOKEN']
                          )
                          print(f'Successfully uploaded {file}!')
                      except Exception as e:
                          print(f'Failed to upload {file}: {e}')
              
              print('Dataset upload completed successfully!')
              
          except Exception as e:
              print(f'Upload failed: {e}')
              print(f'Error type: {type(e).__name__}')
              exit(1)
          "
