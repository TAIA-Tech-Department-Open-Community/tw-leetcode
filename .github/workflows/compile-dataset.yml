name: Compile Dataset

on:
  push:
    branches: [ main ]
    paths:
      - '*/answer.ts'
      - '*/Note.md'
  schedule:
    # Run daily at 12:00 UTC
    - cron: '0 12 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  compile:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for all branches
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install dependencies
        run: |
          cd scripts
          npm install

      - name: Compile dataset
        run: node scripts/compile-dataset.js

      - name: Show dataset stats
        run: |
          echo "Dataset compilation completed!"
          echo "Dataset size: $(wc -l < data/datasets.jsonl) problems"
          echo "File size: $(du -h data/datasets.jsonl | cut -f1)"
          echo ""
          echo "Sample entries:"
          head -2 data/datasets.jsonl | jq '.'

      - name: Validate JSONL format
        run: |
          echo "Validating JSONL format..."
          node -e "
            const fs = require('fs');
            const readline = require('readline');
            
            async function validateJSONL() {
              const fileStream = fs.createReadStream('data/datasets.jsonl');
              const rl = readline.createInterface({
                input: fileStream,
                crlfDelay: Infinity
              });
              
              let lineNumber = 0;
              let validEntries = 0;
              let errors = 0;
              
              for await (const line of rl) {
                lineNumber++;
                try {
                  const data = JSON.parse(line);
                  if (!data.text || !data.question || !data.constraints || !data.thought || !data.answer || !data.src || !data.time_complexity || !data.space_complexity) {
                    console.error(\`Line \${lineNumber}: Missing required fields\`);
                    errors++;
                  } else {
                    validEntries++;
                  }
                } catch (e) {
                  console.error(\`Line \${lineNumber}: Invalid JSON - \${e.message}\`);
                  errors++;
                }
              }
              
              console.log(\`Validation complete: \${validEntries} valid entries, \${errors} errors\`);
              if (errors > 0) {
                process.exit(1);
              }
            }
            
            validateJSONL();
          "

      - name: Create dataset branch
        run: |
          # Create or switch to dataset branch
          git checkout -B dataset
          
          # Backup only the essential files we want to keep
          cp data/datasets.jsonl datasets-backup.jsonl
          [ -f data/dataset_summary.json ] && cp data/dataset_summary.json dataset-summary-backup.json || echo "No dataset_summary.json found"
          cp scripts/README.md.dataset readme-backup.md
          
          # Remove ALL files and directories (including node_modules)
          rm -rf *
          rm -rf .*gitignore .*gitattributes 2>/dev/null || true
          
          # Create clean data directory and restore only the dataset files
          mkdir -p data
          mv datasets-backup.jsonl data/datasets.jsonl
          [ -f dataset-summary-backup.json ] && mv dataset-summary-backup.json data/dataset_summary.json || echo "No summary to restore"
          mv readme-backup.md README.md
          
          # Create clean .gitignore for dataset branch
          echo "# Dataset branch - only contains compiled data" > .gitignore
          echo "node_modules/" >> .gitignore
          echo "*.log" >> .gitignore
          echo ".DS_Store" >> .gitignore
          
          # Create .gitattributes for large files
          echo "*.jsonl filter=lfs diff=lfs merge=lfs -text" > .gitattributes
          echo "data/*.jsonl filter=lfs diff=lfs merge=lfs -text" >> .gitattributes
          
          # Verify clean state
          echo "Files in dataset branch:"
          find . -type f | head -20
          
          # Add and commit files
          git add .
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git commit -m "Update dataset - $(date -u +%Y-%m-%d)" || echo "No changes to commit"
          
          # Push to dataset branch with proper authentication
          git push https://${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git dataset --force

      - name: Setup Python for Hugging Face
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Hugging Face Hub
        run: pip install huggingface_hub

      - name: Switch to dataset branch for upload
        run: git checkout dataset

      - name: Upload to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          # Use the newer hf CLI and add git credentials
          echo "Setting up Hugging Face CLI..."
          hf auth login --token $HF_TOKEN --add-to-git-credential
          
          echo "Current branch: $(git branch --show-current)"
          echo "Files in current directory:"
          find . -type f -not -path './.git/*'
          
          # Create the repository if it doesn't exist
          echo "Creating/checking repository..."
          hf repo create twinkle-ai/tw-leetcode --repo-type dataset --exist-ok || echo "Repository exists or creation skipped"
          
          # Upload files using git-based approach which is more reliable
          echo "Initializing git-lfs and uploading..."
          git lfs install
          
          # Clone the HF repo to a temporary directory
          echo "Cloning HF repository..."
          git clone https://huggingface.co/datasets/twinkle-ai/tw-leetcode hf_repo
          
          # Copy our files to the HF repo
          echo "Copying files to HF repo..."
          cp -r data/ hf_repo/
          cp README.md hf_repo/
          cp .gitattributes hf_repo/ 2>/dev/null || echo "No .gitattributes to copy"
          
          # Commit and push to HF
          echo "Committing and pushing to HF..."
          cd hf_repo
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          git commit -m "Auto-update dataset - $(date -u +%Y-%m-%d)" || echo "No changes to commit"
          git push
          
          echo "Dataset uploaded successfully to Hugging Face!"
